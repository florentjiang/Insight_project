{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# set parameters for text input\n",
    "num_of_samples=10\n",
    "maxlen = 30\n",
    "training_samples = num_of_samples\n",
    "text_vocabulary_size = 10000\n",
    "max_words = text_vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Loading data for movie:  ActOfValorMovie\nLoading data for movie:  Agneepath\nLoading data for movie:  ALateQuartet\nLoading data for movie:  AlbertNobbs\nLoading data for movie:  AlexCrossMovie\nLoading data for movie:  AmourFilm\nLoading data for movie:  AnnaKareninaTheMovie\nLoading data for movie:  arbitragemovie\nLoading data for movie:  ARoyalAffairMovie\nLoading data for movie:  avengers\nLoading data for movie:  Battleship\nLoading data for movie:  BeastsoftheSouthernWild\nLoading data for movie:  bigmiraclemovie\nLoading data for movie:  bobmarleymovie\nLoading data for movie:  boom\nLoading data for movie:  CASAmovie\nLoading data for movie:  celesteandjesseforever\nLoading data for movie:  chasingice\nLoading data for movie:  ChasingMavericks\nLoading data for movie:  chernobyldiaries\nLoading data for movie:  Chronicle\nLoading data for movie:  Contraband\nLoading data for movie:  crookedarrows\nLoading data for movie:  darkshadowsmovie\nLoading data for movie:  DianaVreelandFilm\nLoading data for movie:  DisneynatureChimpanzee\nLoading data for movie:  dreddthemovie\nLoading data for movie:  EkMainAurEkkTu\nLoading data for movie:  EkThaTiger\nLoading data for movie:  endofwatchmovie\nLoading data for movie:  FarewellMyQueenFilm\nLoading data for movie:  FirstPositionDoc\nLoading data for movie:  FiveYearEngagement\nLoading data for movie:  FlightMovie\nLoading data for movie:  Footnote-136341779801626\nLoading data for movie:  forgreaterglory\nLoading data for movie:  Frankenweenie\nLoading data for movie:  FriendsWithKidsMovie\nLoading data for movie:  FunSizeMovie\nLoading data for movie:  ghostridermovie\nLoading data for movie:  GIRLINPROGRESSmovie\nLoading data for movie:  Gone\nLoading data for movie:  GoodDeedsMovie\nLoading data for movie:  GoonFilm\nLoading data for movie:  guilttripmovie\nLoading data for movie:  haywiremovie\nLoading data for movie:  headhuntersfilm\nLoading data for movie:  hitandrunmovie\nLoading data for movie:  HopeSpringsMovie\nLoading data for movie:  HotelT\nLoading data for movie:  HouseAtTheEnd\nLoading data for movie:  HydeParkOnHudson\nLoading data for movie:  Hysteriamovie\nLoading data for movie:  IceAge\nLoading data for movie:  indarknessmovie\nLoading data for movie:  JabTakHaiJaan\nLoading data for movie:  JackReacherMovie\nLoading data for movie:  JamesBond007\nLoading data for movie:  JeffWhoLivesAtHome\nLoading data for movie:  jirodreamsofsushimovie\nLoading data for movie:  JohnCarterMovie\nLoading data for movie:  journey2themysteriousisland\nLoading data for movie:  JoyfulNoise\nLoading data for movie:  KillerJoeTheMovie\nLoading data for movie:  LastOunceofCourage\nLoading data for movie:  LifeofPi\nLoading data for movie:  LincolnMovie\nLoading data for movie:  lockoutmovie\nLoading data for movie:  LooperMovie\nLoading data for movie:  MadagascarMovie\nLoading data for movie:  madeaswitnessprotection\nLoading data for movie:  makerubyreal\nLoading data for movie:  ManOnALedge\nLoading data for movie:  MIBmovie\nLoading data for movie:  monsieurlazharmovie\nLoading data for movie:  MoonriseKingdom\nLoading data for movie:  OddLifeMovie\nLoading data for movie:  oneforthemoneymovie\nLoading data for movie:  paranormalactivity\nLoading data for movie:  ParaNorman\nLoading data for movie:  ParentalGuidance\nLoading data for movie:  peoplelikeus\nLoading data for movie:  PiratesMovie\nLoading data for movie:  pitchperfectmovie\nLoading data for movie:  PixarBrave\nLoading data for movie:  PlayingForKeepsMovie\nLoading data for movie:  PremiumRushMovie\nLoading data for movie:  PromisedLandMovie\nLoading data for movie:  QueenofVersailles\nLoading data for movie:  RedDawnFilm\nLoading data for movie:  Red-Tails-Movie-Fan-Page-116490438450130\nLoading data for movie:  ResidentEvilMovie\nLoading data for movie:  RiseoftheGuardians\nLoading data for movie:  RobotandFrank\nLoading data for movie:  RockofAges\nLoading data for movie:  safemovie\nLoading data for movie:  SafetyNotGuaranteed\nLoading data for movie:  SalmonFishingInTheYemen\nLoading data for movie:  SamsaraFilm\nLoading data for movie:  SearchingForSugarMan\nLoading data for movie:  SecretWorldofArrietty\nLoading data for movie:  seekingafriendmovie\nLoading data for movie:  SilentHillRevelation\nLoading data for movie:  SilentHouse\nLoading data for movie:  SinisterMovie\nLoading data for movie:  SleepwalkMovie\nLoading data for movie:  Sparkle\nLoading data for movie:  TakenMovies\nLoading data for movie:  TakeThisWaltz\nLoading data for movie:  TalaashTheOfficial\nLoading data for movie:  theapparition\nLoading data for movie:  TheCabininTheWoods\nLoading data for movie:  TheColdLightofDay.ma\nLoading data for movie:  thecollectionmovie\nLoading data for movie:  TheDevilInsideMovie\nLoading data for movie:  TheHungerGamesMovie\nLoading data for movie:  TheImpossibleMovie\nLoading data for movie:  theintouchables\nLoading data for movie:  TheKidWithABike\nLoading data for movie:  theloraxuk\nLoading data for movie:  TheLuckyOneMovie\nLoading data for movie:  TheOogieloves\nLoading data for movie:  TheOtherSonFilm\nLoading data for movie:  ThePossessionMovie\nLoading data for movie:  TheRaidUS\nLoading data for movie:  thethreestooges\nLoading data for movie:  TheVow\nLoading data for movie:  thewatchmovie\nLoading data for movie:  TheWomanInBlackMovie\nLoading data for movie:  thewordsmovie\nLoading data for movie:  ThinkLikeAMan\nLoading data for movie:  thismeanswar\nLoading data for movie:  ThousandWordsMovie\nLoading data for movie:  ToRomeWithLoveMovie\nLoading data for movie:  TotalRecall\nLoading data for movie:  TroubleWithTheCurve\nLoading data for movie:  twilight\nLoading data for movie:  unchainedmovie\nLoading data for movie:  unconditionalthemovie\nLoading data for movie:  vampirehunter\nLoading data for movie:  WallflowerMovie\nLoading data for movie:  WhatToExpectMovie\nLoading data for movie:  wimpykidmovie\nLoading data for movie:  WontBackDown\nLoading data for movie:  WrathOfTheTitans\nLoading data for movie:  WreckItRalph\nLoading data for movie:  YourSistersSister\nLoading data for movie:  ZeroDarkThirty\n"
    }
   ],
   "source": [
    "# loading data\n",
    "# X_image_train : image input\n",
    "# X_text_train  : text input\n",
    "# texts         : raw text\n",
    "# y_train_like  : nb of likes output\n",
    "# y_comment_like  : nb of comments output\n",
    "# y_share_like  : nb of shares output\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import pickle\n",
    "pixel_x = 100\n",
    "pixel_y = 100\n",
    "dim = (pixel_x, pixel_y)\n",
    "X_image_train = []\n",
    "y_train_like = []\n",
    "y_train_comment = []\n",
    "y_train_share = []\n",
    "texts = []\n",
    "data_raw=pd.read_csv('page_username.csv')\n",
    "post_list_all_reload = pickle.load( open(\"fb_post_list_1000pages.p\", \"rb\") )\n",
    "for item in data_raw['page_username']:\n",
    "    for movie in post_list_all_reload:\n",
    "        if (movie['username']==item):\n",
    "            print('Loading data for movie: ',item)\n",
    "            for post in movie['posts']:\n",
    "                if post['image'] and post['post_id']:\n",
    "                    fname=movie['username']+'_'+post['post_id']\n",
    "                    fpath = './photos/'+fname+'.jpg'\n",
    "                    try:\n",
    "                        im = cv2.imread(fpath)\n",
    "                        im_resized = cv2.resize(im, dim, interpolation = cv2.INTER_AREA)\n",
    "                        X_image_train.append(im_resized) # loading image data\n",
    "                        texts.append(post['text']) # loading text data\n",
    "                        y_train_like.append(post['likes']+0.1) # loading nb of likes output\n",
    "                        y_train_comment.append(post['comments']+0.1) # loading nb of comments output\n",
    "                        y_train_share.append(post['shares']+0.1) # loading nb of shares output\n",
    "                    except Exception as e:\n",
    "                        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "traing set length: 28858\n"
    }
   ],
   "source": [
    "print('traing set length: {}'.format(len(y_train_like)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Found 40879 unique tokens\n"
    }
   ],
   "source": [
    "# prepare the text input\n",
    "# tokenize raw text into X_text_train\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found {} unique tokens'.format(len(word_index)))\n",
    "X_text_train = pad_sequences(sequences, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Found 400000 word vectors.\n"
    }
   ],
   "source": [
    "# NLP part: using pretrained word embedding model\n",
    "# load the pretrained GloVe coefficient\n",
    "glove_dir = './'\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.50d.txt'),encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "embedding_dim = 50\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if i < max_words:\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP part: build the deep learning model for text input\n",
    "from keras import layers\n",
    "from keras import Input\n",
    "from keras.models import Model\n",
    "from keras import models\n",
    "text_input = Input(shape=(None,), dtype='int32', name='text')\n",
    "embedded_text = layers.Embedding(max_words, embedding_dim)(text_input)\n",
    "encoded_text = layers.LSTM(8)(embedded_text)\n",
    "\n",
    "# Computer vision part: build the deep learning model for image input\n",
    "from keras.applications import VGG16\n",
    "image_input = Input(shape=(pixel_x, pixel_y, 3), name='image')\n",
    "vgg16 = VGG16(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(pixel_x, pixel_y, 3))(image_input)\n",
    "x = layers.Flatten()(vgg16) \n",
    "x = layers.Dense(16, activation='relu')(x)\n",
    "\n",
    "# Concatenate NLP output and computer vision output\n",
    "# build the output layer for regression\n",
    "from keras.optimizers import Adam\n",
    "concatenated = layers.concatenate([x, encoded_text], axis=-1)\n",
    "output = layers.Dense(1, activation=\"linear\")(concatenated)\n",
    "opt = Adam(lr=1e-3, decay=1e-3 / 200)\n",
    "model = Model([image_input, text_input], output)\n",
    "model.compile(loss=\"mean_absolute_percentage_error\", optimizer=opt)\n",
    "\n",
    "# freeze parameters in pretrained models\n",
    "model.layers[1].trainable = False # freeze VGG16 coefficient\n",
    "model.layers[4].set_weights([embedding_matrix])\n",
    "model.layers[4].trainable = False # freeze GloVe word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"model_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nimage (InputLayer)              (None, 100, 100, 3)  0                                            \n__________________________________________________________________________________________________\nvgg16 (Model)                   (None, 3, 3, 512)    14714688    image[0][0]                      \n__________________________________________________________________________________________________\ntext (InputLayer)               (None, None)         0                                            \n__________________________________________________________________________________________________\nflatten_1 (Flatten)             (None, 4608)         0           vgg16[1][0]                      \n__________________________________________________________________________________________________\nembedding_1 (Embedding)         (None, None, 50)     500000      text[0][0]                       \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 16)           73744       flatten_1[0][0]                  \n__________________________________________________________________________________________________\nlstm_1 (LSTM)                   (None, 8)            1888        embedding_1[0][0]                \n__________________________________________________________________________________________________\nconcatenate_1 (Concatenate)     (None, 24)           0           dense_1[0][0]                    \n                                                                 lstm_1[0][0]                     \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 1)            25          concatenate_1[0][0]              \n==================================================================================================\nTotal params: 30,505,033\nTrainable params: 15,290,345\nNon-trainable params: 15,214,688\n__________________________________________________________________________________________________\n"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and save model\n",
    "model.fit([X_image_train, X_text_train], y_train_comment, epochs=2, batch_size=4)\n",
    "model_path=\"./models/\"\n",
    "filename = 'deep_learning_like_model.sav'\n",
    "pickle.dump(model, open(model_path+filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bitf88f055b6d864090b633e218fbc57390",
   "display_name": "Python 3.8.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}